{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1675, 21) (1675, 1) (187, 21) (187, 1)\n"
     ]
    }
   ],
   "source": [
    "def normalization(data):\n",
    "    min_vals = np.min(data, axis=0)\n",
    "    max_vals = np.max(data, axis=0)\n",
    "    _range = max_vals - min_vals\n",
    "    return (data - min_vals) / _range\n",
    "\n",
    "original_data = pd.read_csv('./datasets/AEEEM/ML.csv')       \n",
    "\n",
    "original_data.isnull().values.any()  # Gives false ie:No null value in dataset\n",
    "original_data = original_data.fillna(value=False) \n",
    "original_Y = original_data['class']  \n",
    "original_Y = pd.DataFrame(original_Y)    \n",
    "original_data = normalization(original_data)    \n",
    "\n",
    "original_X = pd.DataFrame(original_data.drop(['class'], axis=1))  \n",
    " \n",
    "x_train, x_test, y_train, y_test = train_test_split(original_X, original_Y, test_size=.1, random_state=12)\n",
    "print(x_train.shape, y_train.shape,x_test.shape, y_test.shape)\n",
    "\n",
    "sm = SMOTE(random_state=12, sampling_strategy=1.0)  \n",
    "x, y = sm.fit_resample(x_train, y_train)  \n",
    "y_train = pd.DataFrame(y, columns=['class'])    \n",
    "x_train = pd.DataFrame(x, columns=original_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2602, 21, 1) (2602, 1) (290, 21, 1) (290, 1) (187, 21, 1) (187, 1)\n"
     ]
    }
   ],
   "source": [
    "#x_test.loc[1,'total_loc'] =  0.264528801 \n",
    "#x_test.loc[1,'comment_loc'] =  0.337140831 \n",
    "#x_test.loc[1,'executable_loc'] =  0.203275228 \n",
    "#x_test.loc[1,'unique_operands'] =  0.369565217 \n",
    "#x_test.loc[1,'unique_operators'] =  0.479770399 \n",
    "#x_test.loc[1,'total_operands'] =  0.282879173 \n",
    "#x_test.loc[1,'total_operators'] =  0.250517298 \n",
    "#x_test.loc[1,'halstead_vocabulary'] =  0.414459875 \n",
    "#x_test.loc[1,'halstead_length'] =  0.262842023 \n",
    "#x_test.loc[1,'halstead_volume'] =  0.244372278 \n",
    "#x_test.loc[1,'halstead_level'] =  0.500382013 \n",
    "#x_test.loc[1,'halstead_difficulty'] =  0.280770116 \n",
    "#x_test.loc[1,'halstead_effort'] =  0.152186111 \n",
    "#x_test.loc[1,'halstead_time'] =  0.152185296 \n",
    "#x_test.loc[1,'branch_count'] =  0.172317183 \n",
    "#x_test.loc[1,'condition_count'] =  0.171775777 \n",
    "#x_test.loc[1,'cyclomatic_complexity'] =  0.179914274 \n",
    "#x_test.loc[1,'cyclomatic_density'] =  0.646325185 \n",
    "#x_test.loc[1,'decision_density'] =  0.290946038 \n",
    "#x_test.loc[1,'design_density'] =  0.200977818 \n",
    "#x_test.loc[1,'normalized_cyclomatic_complexity'] =  0.455357667 \n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=.1, random_state=12)\n",
    "\n",
    "x_train = x_train.values\n",
    "x_val = x_val.values\n",
    "x_test = x_test.values\n",
    "y_train = y_train.values\n",
    "y_val = y_val.values\n",
    "y_test = y_test.values\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2602, 2) (290, 2) (187, 2)\n"
     ]
    }
   ],
   "source": [
    "n_classes = 2\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, n_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, n_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, n_classes)\n",
    "\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float32)\n",
    "x_val = x_val.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: (21, 1)\n"
     ]
    }
   ],
   "source": [
    "input_shape  = (x_train.shape[1], 1)\n",
    "\n",
    "# print(f'input_shape: {input_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat = x_train.reshape((x_train.shape[0], -1))\n",
    "x_val_flat = x_val.reshape((x_val.shape[0], -1))\n",
    "x_test_flat = x_test.reshape((x_test.shape[0], -1))\n",
    "\n",
    "# x_train_flat.shape, x_val_flat.shape, x_test_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = len(x_train_flat[1])\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found on assets/teacher_model.hdf5. Training from scratch.\n",
      "Epoch 1/40\n",
      "82/82 [==============================] - 2s 30ms/step - loss: 0.6479 - acc: 0.6084 - val_loss: 0.6945 - val_acc: 0.5000\n",
      "Epoch 2/40\n",
      "82/82 [==============================] - 2s 23ms/step - loss: 0.6085 - acc: 0.6387 - val_loss: 0.6892 - val_acc: 0.5310\n",
      "Epoch 3/40\n",
      "82/82 [==============================] - 2s 28ms/step - loss: 0.5991 - acc: 0.6603 - val_loss: 0.6943 - val_acc: 0.4621\n",
      "Epoch 4/40\n",
      "82/82 [==============================] - 2s 26ms/step - loss: 0.5957 - acc: 0.6618 - val_loss: 0.6892 - val_acc: 0.5000\n",
      "Epoch 5/40\n",
      "82/82 [==============================] - 2s 25ms/step - loss: 0.5867 - acc: 0.6683 - val_loss: 0.7553 - val_acc: 0.5000\n",
      "Epoch 6/40\n",
      "82/82 [==============================] - 2s 28ms/step - loss: 0.5757 - acc: 0.6695 - val_loss: 0.7022 - val_acc: 0.4966\n",
      "Epoch 7/40\n",
      "82/82 [==============================] - 2s 26ms/step - loss: 0.5630 - acc: 0.6918 - val_loss: 0.7346 - val_acc: 0.5000\n",
      "Epoch 8/40\n",
      "82/82 [==============================] - 2s 25ms/step - loss: 0.5602 - acc: 0.6979 - val_loss: 0.7413 - val_acc: 0.5000\n",
      "Epoch 9/40\n",
      "82/82 [==============================] - 2s 27ms/step - loss: 0.5358 - acc: 0.7110 - val_loss: 0.6892 - val_acc: 0.5828\n",
      "Epoch 10/40\n",
      "82/82 [==============================] - 2s 27ms/step - loss: 0.5216 - acc: 0.7164 - val_loss: 0.6563 - val_acc: 0.6138\n",
      "Epoch 11/40\n",
      "82/82 [==============================] - 2s 26ms/step - loss: 0.5172 - acc: 0.7294 - val_loss: 0.5831 - val_acc: 0.6724\n",
      "Epoch 12/40\n",
      "82/82 [==============================] - 2s 26ms/step - loss: 0.5086 - acc: 0.7375 - val_loss: 0.6435 - val_acc: 0.6207\n",
      "Epoch 13/40\n",
      "82/82 [==============================] - 2s 28ms/step - loss: 0.5013 - acc: 0.7490 - val_loss: 0.5718 - val_acc: 0.6931\n",
      "Epoch 14/40\n",
      "82/82 [==============================] - 2s 26ms/step - loss: 0.5000 - acc: 0.7475 - val_loss: 0.5578 - val_acc: 0.7207\n",
      "Epoch 15/40\n",
      "82/82 [==============================] - 2s 26ms/step - loss: 0.4742 - acc: 0.7629 - val_loss: 0.5479 - val_acc: 0.7483\n",
      "Epoch 16/40\n",
      "82/82 [==============================] - 2s 26ms/step - loss: 0.4720 - acc: 0.7679 - val_loss: 0.5713 - val_acc: 0.7034\n",
      "Epoch 17/40\n",
      "82/82 [==============================] - 2s 27ms/step - loss: 0.4794 - acc: 0.7602 - val_loss: 1.1801 - val_acc: 0.6966\n",
      "Epoch 18/40\n",
      "82/82 [==============================] - 2s 25ms/step - loss: 0.4652 - acc: 0.7759 - val_loss: 1.3699 - val_acc: 0.6862\n",
      "Epoch 19/40\n",
      "82/82 [==============================] - 2s 27ms/step - loss: 0.4412 - acc: 0.7902 - val_loss: 0.8317 - val_acc: 0.6966\n",
      "Epoch 20/40\n",
      "82/82 [==============================] - 2s 27ms/step - loss: 0.4350 - acc: 0.7871 - val_loss: 0.5352 - val_acc: 0.7379\n",
      "Epoch 21/40\n",
      "82/82 [==============================] - 2s 25ms/step - loss: 0.4179 - acc: 0.8036 - val_loss: 0.5195 - val_acc: 0.7586\n",
      "Epoch 22/40\n",
      "82/82 [==============================] - 2s 27ms/step - loss: 0.4176 - acc: 0.7975 - val_loss: 0.5470 - val_acc: 0.7448\n",
      "Epoch 23/40\n",
      "82/82 [==============================] - 2s 28ms/step - loss: 0.4113 - acc: 0.8021 - val_loss: 0.4902 - val_acc: 0.7759\n",
      "Epoch 24/40\n",
      "82/82 [==============================] - 2s 26ms/step - loss: 0.4140 - acc: 0.8040 - val_loss: 0.5708 - val_acc: 0.7207\n",
      "Epoch 25/40\n",
      "82/82 [==============================] - 2s 26ms/step - loss: 0.4047 - acc: 0.8117 - val_loss: 0.4790 - val_acc: 0.7724\n",
      "Epoch 26/40\n",
      "82/82 [==============================] - 2s 28ms/step - loss: 0.4087 - acc: 0.8044 - val_loss: 0.5121 - val_acc: 0.7690\n",
      "Epoch 27/40\n",
      "82/82 [==============================] - 2s 27ms/step - loss: 0.3918 - acc: 0.8140 - val_loss: 0.4689 - val_acc: 0.7724\n",
      "Epoch 28/40\n",
      "82/82 [==============================] - 2s 26ms/step - loss: 0.3894 - acc: 0.8163 - val_loss: 0.5200 - val_acc: 0.7793\n",
      "Epoch 29/40\n",
      "82/82 [==============================] - 2s 27ms/step - loss: 0.3825 - acc: 0.8240 - val_loss: 0.4952 - val_acc: 0.7759\n",
      "Epoch 30/40\n",
      "82/82 [==============================] - 2s 24ms/step - loss: 0.3859 - acc: 0.8136 - val_loss: 0.5775 - val_acc: 0.7379\n",
      "Epoch 31/40\n",
      "82/82 [==============================] - 2s 23ms/step - loss: 0.3746 - acc: 0.8248 - val_loss: 0.8644 - val_acc: 0.7241\n",
      "Epoch 32/40\n",
      "82/82 [==============================] - 2s 28ms/step - loss: 0.3570 - acc: 0.8313 - val_loss: 0.4883 - val_acc: 0.7759\n",
      "Epoch 33/40\n",
      "82/82 [==============================] - 2s 26ms/step - loss: 0.3630 - acc: 0.8344 - val_loss: 0.4928 - val_acc: 0.7655\n",
      "Epoch 34/40\n",
      "82/82 [==============================] - 2s 26ms/step - loss: 0.3538 - acc: 0.8359 - val_loss: 0.4601 - val_acc: 0.7862\n",
      "Epoch 35/40\n",
      "82/82 [==============================] - 2s 27ms/step - loss: 0.3703 - acc: 0.8182 - val_loss: 0.4721 - val_acc: 0.7793\n",
      "Epoch 36/40\n",
      "82/82 [==============================] - 2s 27ms/step - loss: 0.3419 - acc: 0.8444 - val_loss: 0.4622 - val_acc: 0.7862\n",
      "Epoch 37/40\n",
      "82/82 [==============================] - 2s 25ms/step - loss: 0.3548 - acc: 0.8382 - val_loss: 0.4445 - val_acc: 0.7828\n",
      "Epoch 38/40\n",
      "82/82 [==============================] - 2s 26ms/step - loss: 0.3541 - acc: 0.8351 - val_loss: 0.4629 - val_acc: 0.7724\n",
      "Epoch 39/40\n",
      "82/82 [==============================] - 2s 27ms/step - loss: 0.3547 - acc: 0.8359 - val_loss: 0.4666 - val_acc: 0.7862\n",
      "Epoch 40/40\n",
      "82/82 [==============================] - 2s 25ms/step - loss: 0.3389 - acc: 0.8390 - val_loss: 0.4432 - val_acc: 0.7897\n",
      "Saving trained model to assets/teacher_model.hdf5.\n"
     ]
    }
   ],
   "source": [
    "from models.convnet import ConvNet\n",
    "import glob\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 40 \n",
    "\n",
    "'''If you wish to train your own model instead of loading one from checkpoint, remove the checkpoint.'''\n",
    "# for f in glob.glob('assets/teacher_model.hdf5'):\n",
    "#    os.remove(f)\n",
    "\n",
    "nn = ConvNet(input_shape, n_classes=n_classes)   \n",
    " \n",
    "nn.maybe_train(data_train=(x_train, y_train),  \n",
    "               data_valid=(x_val, y_val),  \n",
    "               batch_size=batch_size , epochs=epochs)  \n",
    "\n",
    "nn.evaluate(x_val, y_val)\n",
    "nn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_soft = nn.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found at assets/distilled/. Training from scratch.\n",
      "Starting training...\n",
      "Epoch 1/40\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 5.5323 - accuracy: 0.5719 - val_loss: 5.5106 - val_accuracy: 0.5000\n",
      "Epoch 2/40\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4630 - accuracy: 0.5907 - val_loss: 5.4720 - val_accuracy: 0.5517\n",
      "Epoch 3/40\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4157 - accuracy: 0.6722 - val_loss: 5.4419 - val_accuracy: 0.6103\n",
      "Epoch 4/40\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.3753 - accuracy: 0.7018 - val_loss: 5.4150 - val_accuracy: 0.6207\n",
      "Epoch 5/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.3399 - accuracy: 0.7244 - val_loss: 5.3911 - val_accuracy: 0.6483\n",
      "Epoch 6/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.3079 - accuracy: 0.7375 - val_loss: 5.3689 - val_accuracy: 0.6483\n",
      "Epoch 7/40\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 5.2787 - accuracy: 0.7567 - val_loss: 5.3488 - val_accuracy: 0.6483\n",
      "Epoch 8/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.2521 - accuracy: 0.7533 - val_loss: 5.3312 - val_accuracy: 0.6483\n",
      "Epoch 9/40\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.2274 - accuracy: 0.7571 - val_loss: 5.3138 - val_accuracy: 0.6517\n",
      "Epoch 10/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.2045 - accuracy: 0.7594 - val_loss: 5.2975 - val_accuracy: 0.6552\n",
      "Epoch 11/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.1833 - accuracy: 0.7586 - val_loss: 5.2836 - val_accuracy: 0.6621\n",
      "Epoch 12/40\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 5.1632 - accuracy: 0.7594 - val_loss: 5.2708 - val_accuracy: 0.6586\n",
      "Epoch 13/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.1447 - accuracy: 0.7610 - val_loss: 5.2582 - val_accuracy: 0.6655\n",
      "Epoch 14/40\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.1270 - accuracy: 0.7625 - val_loss: 5.2464 - val_accuracy: 0.6690\n",
      "Epoch 15/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.1107 - accuracy: 0.7644 - val_loss: 5.2359 - val_accuracy: 0.6690\n",
      "Epoch 16/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.0956 - accuracy: 0.7648 - val_loss: 5.2253 - val_accuracy: 0.6724\n",
      "Epoch 17/40\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 5.0808 - accuracy: 0.7663 - val_loss: 5.2160 - val_accuracy: 0.6759\n",
      "Epoch 18/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.0671 - accuracy: 0.7686 - val_loss: 5.2080 - val_accuracy: 0.6759\n",
      "Epoch 19/40\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.0543 - accuracy: 0.7690 - val_loss: 5.1989 - val_accuracy: 0.6793\n",
      "Epoch 20/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.0421 - accuracy: 0.7679 - val_loss: 5.1924 - val_accuracy: 0.6724\n",
      "Epoch 21/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.0306 - accuracy: 0.7679 - val_loss: 5.1830 - val_accuracy: 0.6793\n",
      "Epoch 22/40\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 5.0196 - accuracy: 0.7683 - val_loss: 5.1769 - val_accuracy: 0.6759\n",
      "Epoch 23/40\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.0092 - accuracy: 0.7694 - val_loss: 5.1698 - val_accuracy: 0.6828\n",
      "Epoch 24/40\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.9994 - accuracy: 0.7698 - val_loss: 5.1635 - val_accuracy: 0.6862\n",
      "Epoch 25/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 4.9900 - accuracy: 0.7694 - val_loss: 5.1578 - val_accuracy: 0.6862\n",
      "Epoch 26/40\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 4.9813 - accuracy: 0.7671 - val_loss: 5.1520 - val_accuracy: 0.6828\n",
      "Epoch 27/40\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 4.9727 - accuracy: 0.7679 - val_loss: 5.1469 - val_accuracy: 0.6862\n",
      "Epoch 28/40\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.9646 - accuracy: 0.7679 - val_loss: 5.1418 - val_accuracy: 0.6828\n",
      "Epoch 29/40\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.9569 - accuracy: 0.7659 - val_loss: 5.1353 - val_accuracy: 0.6862\n",
      "Epoch 30/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 4.9493 - accuracy: 0.7667 - val_loss: 5.1309 - val_accuracy: 0.6828\n",
      "Epoch 31/40\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 4.9421 - accuracy: 0.7663 - val_loss: 5.1264 - val_accuracy: 0.6828\n",
      "Epoch 32/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 4.9353 - accuracy: 0.7675 - val_loss: 5.1214 - val_accuracy: 0.6862\n",
      "Epoch 33/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 4.9288 - accuracy: 0.7659 - val_loss: 5.1172 - val_accuracy: 0.6828\n",
      "Epoch 34/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 4.9224 - accuracy: 0.7667 - val_loss: 5.1121 - val_accuracy: 0.6897\n",
      "Epoch 35/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 4.9163 - accuracy: 0.7663 - val_loss: 5.1079 - val_accuracy: 0.6897\n",
      "Epoch 36/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 4.9105 - accuracy: 0.7663 - val_loss: 5.1027 - val_accuracy: 0.6897\n",
      "Epoch 37/40\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 4.9048 - accuracy: 0.7667 - val_loss: 5.0995 - val_accuracy: 0.6897\n",
      "Epoch 38/40\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.8995 - accuracy: 0.7679 - val_loss: 5.0955 - val_accuracy: 0.6897\n",
      "Epoch 39/40\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.8942 - accuracy: 0.7679 - val_loss: 5.0925 - val_accuracy: 0.6828\n",
      "Epoch 40/40\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.8891 - accuracy: 0.7659 - val_loss: 5.0878 - val_accuracy: 0.6931\n",
      "Training completed.\n",
      "Saving trained model to assets/distilled/tree-model.\n"
     ]
    }
   ],
   "source": [
    "from models.tree import SoftDecisionTree\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "learning_rate = 3e-4  \n",
    "max_depth = 4\n",
    "n_features = n_features\n",
    "n_classes = 2\n",
    "penalty_strength = 1e+1\n",
    "penalty_decay = 0.25\n",
    "inv_temp = 0.01  \n",
    "epochs = 40\n",
    "ema_win_size = 100\n",
    "batch_size = 16\n",
    "\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "tree_model = SoftDecisionTree(max_depth=max_depth, n_features=n_features, n_classes=n_classes, \n",
    "                          penalty_strength=penalty_strength, penalty_decay=penalty_decay,  \n",
    "                          inv_temp=inv_temp, ema_win_size=ema_win_size) \n",
    " \n",
    "data_train = x_train_flat, y_train  \n",
    "data_val =  x_val_flat, y_val  \n",
    "\n",
    "tree_model.tree_train(x_train_flat, y_train_soft, x_val_flat, y_val, batch_size, epochs , distill=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss   = 12.62952525799091   #53.85\n",
    "loss1  = 12.629525331350473\n",
    "loss2  = 12.629525184631348\n",
    "loss3  = 12.629525111271786\n",
    "loss4  = 12.629525184631348\n",
    "loss5  = 12.629525331350473\n",
    "loss6  = 12.629525184631348\n",
    "loss7  = 12.629525331350473\n",
    "loss8  = 12.629525331350473\n",
    "loss9  = 12.62952525799091\n",
    "loss10 = 12.62952525799091\n",
    "loss11 = 12.629525184631348\n",
    "loss12 = 12.629525184631348\n",
    "loss13 = 12.629525331350473\n",
    "loss14 = 12.629525331350473\n",
    "loss15 = 12.629525404710035\n",
    "loss16 = 12.62952525799091\n",
    "loss17 = 12.62952525799091\n",
    "loss18 = 12.629525184631348\n",
    "loss19 = 12.629525331350473    #46.15% \n",
    "loss20 = 12.629525184631348\n",
    "loss21 = 12.62952525799091\n",
    "\n",
    "Sensitivity1 = (loss1 - loss) / 0.21076536\n",
    "Sensitivity2 = (loss2 - loss) / 0.203807498\n",
    "Sensitivity3 = (loss3 - loss) / 0.190775228\n",
    "Sensitivity4 = (loss4 - loss) / 0.217391304\n",
    "Sensitivity5 = (loss5 - loss) / 0.244476282\n",
    "Sensitivity6 = (loss6 - loss) / 0.205956096\n",
    "Sensitivity7 = (loss7 - loss) / 0.200793541\n",
    "Sensitivity8 = (loss8 - loss) / 0.228019197\n",
    "Sensitivity9 = (loss9 - loss) / 0.202439338\n",
    "Sensitivity10 = (loss10 - loss) / 0.199596158\n",
    "Sensitivity11 = (loss11 - loss) / 0.252959332\n",
    "Sensitivity12 = (loss12 - loss) / 0.197256351\n",
    "Sensitivity13 = (loss13 - loss) / 0.146958898\n",
    "Sensitivity14 = (loss14 - loss) / 0.146958831\n",
    "Sensitivity15 = (loss15 - loss) / 0.172317183\n",
    "Sensitivity16 = (loss16 - loss) / 0.171775777\n",
    "Sensitivity17 = (loss17 - loss) / 0.179914274\n",
    "Sensitivity18 = (loss18 - loss) / 0.22079327\n",
    "Sensitivity19 = (loss19 - loss) / 0.290946038\n",
    "Sensitivity20 = (loss20 - loss) / 0.200977818\n",
    "Sensitivity21 = (loss21 - loss) / 0.221315114\n",
    "\n",
    "print(Sensitivity1)\n",
    "print(Sensitivity2)\n",
    "print(Sensitivity3)\n",
    "print(Sensitivity4)\n",
    "print(Sensitivity5)\n",
    "print(Sensitivity6)\n",
    "print(Sensitivity7)\n",
    "print(Sensitivity8)\n",
    "print(Sensitivity9)\n",
    "print(Sensitivity10)\n",
    "print(Sensitivity11)\n",
    "print(Sensitivity12)\n",
    "print(Sensitivity13)\n",
    "print(Sensitivity14)\n",
    "print(Sensitivity15)\n",
    "print(Sensitivity16)\n",
    "print(Sensitivity17)\n",
    "print(Sensitivity18)\n",
    "print(Sensitivity19)\n",
    "print(Sensitivity20)\n",
    "print(Sensitivity21)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
